{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfefe9b5-c47e-44bb-8a5f-a5fa2b5a9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de104d6c-8f89-452a-b733-f7f8ed88b17d",
   "metadata": {},
   "source": [
    "# Event Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc0f16a1-0097-4e42-ba35-61aec0aad571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      21.468782\n",
       "After      -6.221490\n",
       "t_After    -0.329245\n",
       "t          -0.329274\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(1000)\n",
    "t0 = 500\n",
    "Y = 2 * (t < t0) + 1 * (t >= t0) + np.random.normal(0, 1, (1000,))\n",
    "\n",
    "After = 1 * (t >= t0)\n",
    "t_After = (t - t0) * After\n",
    "# intercept (before), After, before trend (t), after trend (t - t0) * After\n",
    "\n",
    "X = sm.add_constant(pd.DataFrame({\"After\": After, \"t_After\": t_After, \"t\": t}))\n",
    "results = sm.OLS(Y, X).fit()\n",
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a146f4-e707-4c28-93e4-b01f3a15f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      1.869648\n",
       "After     -0.969933\n",
       "t_After   -0.001333\n",
       "t          0.000493\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scenario: after a new grocery store opens up, does our (old) store reduce the price of a pound of apples?\n",
    "# First experiment: yes, it reduces\n",
    "t = np.arange(1000)\n",
    "t0 = 500\n",
    "Y = 2 * (t < t0) + 1 * (t >= t0) + np.random.normal(0, 1, (1000,))\n",
    "# Test\n",
    "After = 1 * (t >= 500)\n",
    "t_After = (t - t0) * After\n",
    "X = sm.add_constant(pd.DataFrame({\"After\": After, \"t_After\": t_After, \"t\": t}))\n",
    "results = sm.OLS(Y, X).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6796195-96ba-4a09-93d5-6496aedb21bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      20.731726\n",
       "After      -7.593641\n",
       "t_After    -3.011822\n",
       "t           1.574741\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e16f324-9461-4d8a-b345-ffab8b28d244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      2.825656e-03\n",
       "After      3.996087e-03\n",
       "t_After    1.384285e-08\n",
       "t          9.788375e-09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's define a function to do this for us.\n",
    "def event_study(num_times, before_const, after_const, before_slope, after_slope, sigma):\n",
    "    t = np.arange(num_times)\n",
    "    t0 = int(num_times / 2)\n",
    "    Y = before_const * (t < t0) + before_slope * t + after_const * (t >= t0) + (after_slope - before_slope) * (t - t0) * (t >= t0) + np.random.normal(0, sigma, (num_times,))\n",
    "    # Test\n",
    "    After = 1 * (t >= t0)\n",
    "    t_After = (t - t0) * After\n",
    "    X = sm.add_constant(pd.DataFrame({\"After\": After, \"t_After\": t_After, \"t\": t}))\n",
    "    results = sm.OLS(Y, X).fit()\n",
    "    return results\n",
    "\n",
    "results = event_study(num_times = 1000000, before_const = 2.0, after_const = 1.0, before_slope = 0.01, after_slope = 0, sigma = 1)\n",
    "results.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb29fb3-9abf-439b-b8d7-03b86d003975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e88653f2-24fc-478f-a0d0-55f00ff97066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      1.996522e+00\n",
       "After     -9.923754e-02\n",
       "t_After   -4.533106e-07\n",
       "t          9.778994e-08\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = event_study(num_times = 100000, before_const = 2.0, after_const = 1.9, before_slope = 0, after_slope = 0, sigma = 1)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e66d39c-8cdb-45dc-ba9b-62c12c3a3930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      223.104221\n",
       "After       -7.841301\n",
       "t_After     -1.033992\n",
       "t            0.315450\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e3fc00-d283-4520-9a07-ad9d16c8094d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      1.967959\n",
       "After     -0.044452\n",
       "t_After    1.000094\n",
       "t         -0.000088\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = event_study(num_times = 1000, before_const = 2.0, after_const = 1.9, before_slope = 0, after_slope = 1, sigma = 1)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cee53d-5600-4239-91b7-3dc6331d4428",
   "metadata": {},
   "source": [
    "# Differences-in-differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2043eb5a-a366-473d-9aca-ce1cd1357048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const          1.531035\n",
       "alpha_group    0.418678\n",
       "alpha_time    -0.010358\n",
       "beta_1        -0.960591\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scenario: a new grocery store opens up in town A but not town B.  Do the stores in town A reduce the price of a pound of apples\n",
    "# because of this event?  Or do they reduce it for other reasons?\n",
    "\n",
    "# First experiment: yes, it reduces\n",
    "t = np.arange(1000)\n",
    "t0 = 500\n",
    "Y_A = 2 * (t < t0) + 1 * (t >= t0) + np.random.normal(0, 1, (1000,))\n",
    "Y_B = 1.5 + np.random.normal(0, 1, (1000,))\n",
    "Y = np.concatenate((Y_A, Y_B))\n",
    "group_A = np.concatenate((np.ones(1000), np.zeros(1000)))\n",
    "After = np.tile(1 * (t >= t0), 2)\n",
    "Treated = group_A * After\n",
    "X = sm.add_constant(pd.DataFrame({\"alpha_group\": group_A, \"alpha_time\": After, \"beta_1\": Treated}))\n",
    "results = sm.OLS(Y, X).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "835aa9d2-e7ac-4402-9c12-ff3642631d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const          1.948211\n",
       "alpha_group   -0.966374\n",
       "alpha_time     1.094004\n",
       "beta_1        -0.140652\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DID(num_times, before_const_A, after_const_A, before_const_B, after_const_B, sigma):\n",
    "    t = np.arange(num_times)\n",
    "    t0 = int(num_times / 2)\n",
    "    Y_A = before_const_A * (t < t0) + after_const_A * (t >= t0) + np.random.normal(0, sigma, (num_times,))\n",
    "    Y_B = before_const_B * (t < t0) + after_const_B * (t >= t0) + np.random.normal(0, sigma, (num_times,))\n",
    "    Y = np.hstack((Y_A, Y_B))\n",
    "    group_A = np.hstack((np.ones(num_times), np.zeros(num_times)))\n",
    "    After = np.hstack((1 * (t >= t0), 1 * (t >= t0)))\n",
    "    X = sm.add_constant(pd.DataFrame({\"alpha_group\": group_A, \"alpha_time\": After, \"beta_1\": group_A * After}))\n",
    "    results = sm.OLS(Y, X).fit()\n",
    "    return results\n",
    "\n",
    "results = DID(num_times = 1000, before_const_A = 1.0, after_const_A = 2.0, before_const_B = 2.0, after_const_B = 3.0, sigma = 1)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca1c2835-d6cc-4eab-9606-1d8ebff0da36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const          44.911507\n",
       "alpha_group   -16.342896\n",
       "alpha_time     16.012549\n",
       "beta_1         -0.105436\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18367e39-25a5-4e82-bc4e-2178f60c07f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const          1.982108\n",
       "alpha_group   -1.070624\n",
       "alpha_time     0.971736\n",
       "beta_1         1.183515\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = DID(num_times = 1000, before_const_A = 1.0, after_const_A = 3.0, before_const_B = 2.0, after_const_B = 3.0, sigma = 1)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed74aa1b-c3a4-497d-9cd2-fd3468eb9d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE) using IPW: 2.274\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"homework_8.1.csv\")\n",
    "\n",
    "# Step 2: Fit logistic regression (Z -> X) to estimate propensity scores\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(df[['Z']], df['X'])\n",
    "\n",
    "# Step 3: Predict propensity scores (P(X=1 | Z))\n",
    "df['propensity_score'] = logreg.predict_proba(df[['Z']])[:, 1]\n",
    "\n",
    "# Step 4: Calculate inverse probability weights (IPW)\n",
    "df['ipw'] = np.where(df['X'] == 1,\n",
    "                     1 / df['propensity_score'],\n",
    "                     1 / (1 - df['propensity_score']))\n",
    "\n",
    "# Step 5: Compute weighted average outcomes for treated and control groups\n",
    "treated = df[df['X'] == 1]\n",
    "control = df[df['X'] == 0]\n",
    "\n",
    "weighted_y_treated = np.sum(treated['Y'] * treated['ipw']) / np.sum(treated['ipw'])\n",
    "weighted_y_control = np.sum(control['Y'] * control['ipw']) / np.sum(control['ipw'])\n",
    "\n",
    "# Step 6: Compute Average Treatment Effect (ATE)\n",
    "ate_ipw = weighted_y_treated - weighted_y_control\n",
    "print(\"Average Treatment Effect (ATE) using IPW:\", round(ate_ipw, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39777aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three propensity scores: [0.84 0.58 0.71]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"homework_8.1.csv\")\n",
    "\n",
    "# Step 2: Fit logistic regression model (Z -> X)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(df[['Z']], df['X'])\n",
    "\n",
    "# Step 3: Predict propensity scores\n",
    "df['propensity_score'] = logreg.predict_proba(df[['Z']])[:, 1]\n",
    "\n",
    "# Step 4: Show the first three propensity scores\n",
    "first_three_scores = df['propensity_score'].head(3).values\n",
    "print(\"First three propensity scores:\", np.round(first_three_scores, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612f7760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE) using Mahalanobis matching: 3.414\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "# Step 2: Split treated and untreated groups\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "control = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Compute inverse covariance matrix from Z1 and Z2 of control group\n",
    "Z_control = control[['Z1', 'Z2']].values\n",
    "cov_matrix = np.cov(Z_control.T)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Step 4: For each treated item, find the nearest untreated item using Mahalanobis distance\n",
    "matched_outcomes = []\n",
    "for _, treat_row in treated.iterrows():\n",
    "    z_treat = treat_row[['Z1', 'Z2']].values\n",
    "    distances = [mahalanobis(z_treat, z_control, inv_cov_matrix) for z_control in Z_control]\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    matched_outcomes.append(control.iloc[nearest_idx]['Y'])\n",
    "\n",
    "# Step 5: Calculate ATE\n",
    "treated_mean = treated['Y'].mean()\n",
    "control_matched_mean = np.mean(matched_outcomes)\n",
    "ate = treated_mean - control_matched_mean\n",
    "\n",
    "print(\"Average Treatment Effect (ATE) using Mahalanobis matching:\", round(ate, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a800f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 and Z2 of the treated item with least common support: [2.7  0.54]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "# Step 2: Split treated and untreated groups\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "control = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Compute inverse covariance matrix from control group's Z1 and Z2\n",
    "Z_control = control[['Z1', 'Z2']].values\n",
    "cov_matrix = np.cov(Z_control.T)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Step 4: Find the treated unit with the maximum minimum Mahalanobis distance to control\n",
    "max_distance = -np.inf\n",
    "max_index = -1\n",
    "\n",
    "for i, treat_row in treated.iterrows():\n",
    "    z_treat = treat_row[['Z1', 'Z2']].values\n",
    "    distances = [mahalanobis(z_treat, z_control, inv_cov_matrix) for z_control in Z_control]\n",
    "    min_dist = np.min(distances)\n",
    "    if min_dist > max_distance:\n",
    "        max_distance = min_dist\n",
    "        max_index = i\n",
    "\n",
    "# Step 5: Output the Z1 and Z2 of the treated item with least common support\n",
    "least_supported_point = treated.iloc[max_index][['Z1', 'Z2']].values\n",
    "print(\"Z1 and Z2 of the treated item with least common support:\", np.round(least_supported_point, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df228d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE) using Mahalanobis matching: 3.414\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "# Step 2: Split into treated and control groups\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "control = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Compute inverse covariance matrix of Z1 and Z2 from the control group\n",
    "Z_control = control[['Z1', 'Z2']].values\n",
    "cov_matrix = np.cov(Z_control.T)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Step 4: Match each treated unit to the nearest control using Mahalanobis distance (with replacement)\n",
    "matched_outcomes = []\n",
    "for _, treat_row in treated.iterrows():\n",
    "    z_treat = treat_row[['Z1', 'Z2']].values\n",
    "    distances = [mahalanobis(z_treat, z_control, inv_cov_matrix) for z_control in Z_control]\n",
    "    matched_index = np.argmin(distances)\n",
    "    matched_outcomes.append(control.iloc[matched_index]['Y'])\n",
    "\n",
    "# Step 5: Compute ATE\n",
    "treated_mean = treated['Y'].mean()\n",
    "matched_control_mean = np.mean(matched_outcomes)\n",
    "ate = treated_mean - matched_control_mean\n",
    "\n",
    "# Step 6: Output the result\n",
    "print(\"Average Treatment Effect (ATE) using Mahalanobis matching:\", round(ate, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9919eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treated item with least common support (Z1, Z2): [2.7  0.54]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "# Step 2: Split treated and control groups\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "control = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Compute inverse covariance matrix of control Z1 and Z2\n",
    "Z_control = control[['Z1', 'Z2']].values\n",
    "cov_matrix = np.cov(Z_control.T)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Step 4: Find the treated item farthest from its nearest control (least common support)\n",
    "max_distance = -np.inf\n",
    "max_index = -1\n",
    "\n",
    "for i, treat_row in treated.iterrows():\n",
    "    z_treat = treat_row[['Z1', 'Z2']].values\n",
    "    distances = [mahalanobis(z_treat, z_control, inv_cov_matrix) for z_control in Z_control]\n",
    "    min_dist = np.min(distances)\n",
    "    if min_dist > max_distance:\n",
    "        max_distance = min_dist\n",
    "        max_index = i\n",
    "\n",
    "# Step 5: Output the Z1 and Z2 values of the least supported treated item\n",
    "least_supported_treated = treated.iloc[max_index][['Z1', 'Z2']]\n",
    "print(\"Treated item with least common support (Z1, Z2):\", np.round(least_supported_treated.values, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c0bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
